---
title: "Assets Related to Solids Train"
author: "Jane Ho"
format:
  html:
    embed-resources: true
    code-fold: true
execute:
  warning: false
---


# Readme - conceptual steps  

*2025.11.12 (last data download/update)*  


1. Download Assets table from Maximo, filtered for wastewater facilities using %WW% in location  
2. Find relevant assets in description field - DIGESTER, DEWATER, THICKEN, CENTRIFUGE, SLUDGE TANK, SLUDGE STORAGE, BIOSOLID TANK, BIOSOLID STORAGE, LAGOON  
3. Combine redundant terms by location (4 digit id under Location field)  
4. Simplify location to plant name (usually the string after first comma, some cases like Sanofi are standalone)
N.B. Region of Waterloo not present in current Maximo extraction

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(leaflet)
library(janitor)
library(tidyr)
library(dplyr)
library(readxl)
library(purrr)
library(ggplot2)
library(viridis)
library(stringr)
library(lubridate)
library(svglite)
library(fuzzyjoin)
library(forcats)
library(DT)
library(gt)
library(tidygeocoder)
options(scipen = 999) 

# Capture the code run date/time
code_run_date <- Sys.Date()
code_run_time <- format(Sys.time(), "%Y-%m-%d %H:%M:%S")
```

# Import
```{r}
#grab all excel names as a character vector, assuming multiple quarterly files will be available.  may need more precision if non wiski files are stored

# the data file is queried with only "%WW%" to limit to ww assets 
fnames <- fs::dir_ls('data/2-Facilities/', regexp = 'maximoWWassets.xlsx')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_maximoAssets <- suppressMessages(suppressWarnings(
  bind_rows(lapply(fnames, read_excel, col_names = TRUE, trim_ws=TRUE))
))

#plant addresses
fnames <- fs::dir_ls('data/2-Facilities/', regexp = 'maximoLocation.xlsx')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_maximoLocation <- suppressMessages(suppressWarnings(
  bind_rows(lapply(fnames, read_excel, col_names = TRUE, trim_ws=TRUE))
))

#there are two fields named Description - change
dt_maximoAssets<- suppressMessages(suppressWarnings(
  dt_maximoAssets %>% 
    rename(asset_description = 2, loc_description = 4) %>% #asset description 
    select(-Site, -Parent) %>% # no info in these
    janitor::clean_names() #asset description 
))

#report number of rows
cat("\nEntries downloaded from Maximo:\n")
print(nrow(dt_maximoAssets))
```


```{r}
#reduce & normalize dataset

#set search strings
keywords<- c("DIGESTER", "DEWATER", "THICKEN", "CENTRIFUGE", "SLUDGE TANK", "SLUDGE STORAGE", "BIOSOLID TANK", "BIOSOLID STORAGE", "LAGOON")

combined_keywords <- paste(keywords, collapse = "|")

dt_maximoAssets_wwSolids<-dt_maximoAssets %>%
  # find solids train related
  filter(grepl(combined_keywords, asset_description)) %>%
  # include any operating, inactive, not in contract or not ready
  filter(!grepl("DECOMMISSIONED", status)) %>% 
  # get location code and human readable loc
  mutate(loc_code = substr(location, start = 1, stop = 4))%>%
  mutate(
    # Split into 3 columns: Before first comma, Between 1st and 2nd, After 2nd/Last
    parts = str_split_fixed(loc_description, pattern = ",", n = 3),
    # The middle part is readable plant name; if no comma, use the whole string
    loc_name = if_else(
      grepl(",", loc_description),
      str_trim(parts[, 2]),
      str_trim(loc_description)
    )
  )%>%
  select(-parts) # Clean up temporary columns

  # Extract the first occurrence of any matched keyword for each row
dt_maximoAssets_wwSolids<-dt_maximoAssets_wwSolids %>%
  mutate(matched_keyword = str_extract(asset_description, combined_keywords)) %>%
  # Keep only the first instance of each matched_keyword at each loc_description
  group_by(loc_name, matched_keyword) %>%
  slice(1) %>%
  ungroup()
```

```{r}
# Normalize column names and attempt to parse addresses into lat/lon
dt_maximoLocation <- dt_maximoLocation %>% janitor::clean_names()
# Normalize common state/province abbreviations (missing or ON -> Ontario, QC -> Quebec)
#there's also a consultant in MB?
dt_maximoLocation <- dt_maximoLocation %>%
  mutate(state_province = ifelse(
    !is.na(state_province) & str_to_upper(str_squish(state_province)) == "ON",
    "Ontario",
    state_province
  )) %>%
  mutate(state_province = ifelse(
    str_to_upper(str_squish(state_province)) == "QC",
    "Quebec",
    state_province))  %>%
  mutate(country = "Canada")

# Create new table for geocoding, Drop rows where state_province is missing; some have street addresses but need extra grease to break
dt_maximoGeoLocation <- dt_maximoLocation %>%
  filter(!is.na(state_province))

# street addresses have artefacts to be removed, then remove the leftover comma / whitepace
dt_maximoGeoLocation$street_address<- gsub("ON.*","", 
  gsub("PO.*","", 
  gsub("Suite.*","",dt_maximoGeoLocation$street_address)))
dt_maximoGeoLocation$street_address <- sub(",\\s*$", "", dt_maximoGeoLocation$street_address)
dt_maximoGeoLocation$street_address<-trimws(dt_maximoGeoLocation$street_address)

#custom removal of certain towns in street_address
dt_maximoGeoLocation$street_address <- sub(",\\sTobermory$", "", dt_maximoGeoLocation$street_address)
dt_maximoGeoLocation$street_address <- sub(",\\sDelhi$", "", dt_maximoGeoLocation$street_address)
dt_maximoGeoLocation$street_address <- sub(",\\sAmherstburg$", "", dt_maximoGeoLocation$street_address)

#Passing addresses to the Nominatim single address geocoder, https://jessecambon.github.io/tidygeocoder/articles/tidygeocoder.html
#Only unique input data (either addresses or coordinates) is passed to geocoding services even if your data contains duplicates. NA and blank inputs are excluded from queries. Input latitudes and longitudes are also limited to the range of possible values.
dt_maximoGeoLocation <- dt_maximoGeoLocation %>%  geocode(street = street_address, city = city, state = state_province, country = country, method = "osm") #, verbose = TRUE # doesn't tell you which failed though

# add a failure catch  for NAs to look just at town state country - total capture now 267 out of 281 instead of 158
dt_maximoGeoLocation_townOnly <- dt_maximoGeoLocation[is.na(dt_maximoGeoLocation$lat),] %>% select(-lat, -long)  %>% geocode(city = city, state = state_province, country = country, method = "osm")

dt_maximoGeoLocation<-rbind(dt_maximoGeoLocation[!is.na(dt_maximoGeoLocation$lat),], dt_maximoGeoLocation_townOnly)

write.csv(dt_maximoGeoLocation, "dt_maximoGeolocation.csv", row.names = FALSE)
```

```{r}
# Test for NAs in loc_code and loc_name
cat("NAs in loc_code:", sum(is.na(dt_maximoAssets_wwSolids$loc_code)), "\n")
cat("NAs in loc_name:", sum(is.na(dt_maximoAssets_wwSolids$loc_name)), "\n")
cat("Total rows:", nrow(dt_maximoAssets_wwSolids), "\n")

# Show any rows with NAs
if (any(is.na(dt_maximoAssets_wwSolids$loc_code)) || any(is.na(dt_maximoAssets_wwSolids$loc_name))) {
  cat("\nRows with NAs:\n")
  print(dt_maximoAssets_wwSolids %>% filter(is.na(loc_code) | is.na(loc_name)) %>% select(location, loc_description, loc_code, loc_name))
}
```

# Tabulate loc_codes by matched_keyword
```{r}
# Count unique loc_codes for each matched_keyword
dt_equipment_count <- dt_maximoAssets_wwSolids %>%
  filter(!is.na(matched_keyword)) %>%
  group_by(matched_keyword) %>%
  summarise(
    equippedWWTPs = n_distinct(loc_code), .groups = 'drop') %>%
  arrange(desc(equippedWWTPs))

dt_equipment_count %>%
  gt() %>%
  cols_label(
    matched_keyword = "Equipment Type",
    equippedWWTPs = "Number of WWTPs"
  ) %>%
  tab_header(
    title = "Solids-Related Equipment Counts",
    subtitle = paste("Data as of:", format(code_run_date, "%B %d, %Y"))
  ) %>%
  opt_stylize(style = 6, color = "blue")
```

```{r fig.width=12, fig.height=24}
#heatmap showing which keywords are present in each location

dt_maximoAssets_wwSolids<- dt_maximoAssets_wwSolids %>% mutate(loc_name = fct_reorder(loc_name, desc(loc_name)))

p_maximoAssets_wwSolids<-
  dt_maximoAssets_wwSolids %>%
    group_by(loc_name, matched_keyword) %>%
    summarise(count = n(), .groups = 'drop') %>%
    ggplot(aes(x = matched_keyword, y = reorder(loc_name, loc_name, decreasing = TRUE), fill = count)) +
    geom_tile(color = "white", linewidth = 0.5) +
    scale_fill_viridis_c(option = "cividis", direction = -1) +
    theme_minimal() +
    labs(
      title = "Solids-Related Asset in WWTPs (Maximo )",
      x = "Equipment Type",
      y = "Location",
      fill = "Count",
      caption = paste("Code run on:", format(code_run_date, "%B %d, %Y"), "at", code_run_time)
    ) +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1),
      plot.title = element_text(size = 14, face = "bold"),
      plot.caption = element_text(size = 9, hjust = 0, face = "italic"),
      legend.position = "none"
    )

p_maximoAssets_wwSolids
```

# Filterable Table
*(interactivity - filter, sort, search are all broken once loaded to sharepoint - use ctrl+f in lieu...)*

```{r, eval = TRUE}
  # Create a gt version of the same table - doesn't work on sharepoint (bc of interactivity?)
  dt_maximoAssets_wwSolids %>%
    select(loc_code, loc_name, matched_keyword) %>%
    arrange(loc_name, matched_keyword) %>%
    mutate(loc_code = as.character(loc_code)) %>%
    gt() %>%
    cols_label(
      loc_code = 'Maximo Location Code',
      loc_name = 'Location Name',
      matched_keyword = 'Solids Train Key Asset'
    ) %>%
    cols_align(align = "left", columns = everything()) %>%
    tab_header(
      title = 'Solids Assets (by Location)'
    ) %>%
    # put the data-run date in a footnote/source note
    tab_source_note(md(paste('Data as of:', format(code_run_date, '%B %d, %Y')))) %>%
    fmt_missing(columns = everything(), missing_text = '-') %>%
    tab_options(table.font.names = 'Arial', heading.align = 'left')
```
```{r, eval = FALSE}
# Create a filterable table with key columns - DT does not work at in Sharepoint
dt_maximoAssets_wwSolids %>%
  select(loc_code, loc_name, matched_keyword) %>%
  arrange(loc_name, matched_keyword) %>%
  datatable(
    filter = 'top',
    options = list(
      pageLength = 15,
      autoWidth = TRUE,
      columnDefs = list(
        list(width = '100px', targets = c(0, 2)),
        list(width = '150px', targets = 1)
      )
    ),
    colnames = c('Maximo Location Code', 'Location Name', 'Solids Train Key Asset'),
    caption = htmltools::tags$caption(
      style = 'caption-side: bottom; text-align: left;',
      paste("Click on column headers to sort; use search boxes to filter | Data as of:", format(code_run_date, "%B %d, %Y"))
    )
  )
```
```{r, eval = FALSE}
# Create a gt version of the same table - doesn't work on sharepoint (bc of interactivity?)
dt_maximoAssets_wwSolids %>%
  select(loc_code, loc_name, matched_keyword) %>%
  arrange(loc_name, matched_keyword) %>%
  mutate(loc_code = as.character(loc_code)) %>%
  gt() %>%
  cols_label(
    loc_code = 'Maximo Location Code',
    loc_name = 'Location Name',
    matched_keyword = 'Solids Train Key Asset'
  ) %>%
  cols_align(align = "left", columns = everything()) %>%
  tab_header(
    title = 'Solids Assets (by Location)'
  ) %>%
  # enable interactive features (sorting/searching) for all columns
  opt_interactive() %>%
  # put the data-run date in a footnote/source note
  tab_source_note(md(paste('Data as of:', format(code_run_date, '%B %d, %Y')))) %>%
  fmt_missing(columns = everything(), missing_text = '-') %>%
  tab_options(table.font.names = 'Arial', heading.align = 'left')
```

```{r}
# export as a table with all needed fields to create nested json for leaflet (cannot use r generated leaflet map in sharepoint)

# Extract the first 4 loc characters for joining
dt_maximoGeoLocation <- dt_maximoGeoLocation %>%
  mutate(loc_code = substr(address_code, 1, 4))
  
# pivot wide with boolean of each equipment field then the name, loc code 
dt_maximoAssets_wwSolidsLoc <- dt_maximoAssets_wwSolids %>%
  mutate(present = 1) %>% #placeholder to allow pivot boolean
  pivot_wider(id_cols = c(loc_name, location, asset, loc_code), names_from = matched_keyword, values_from = present, values_fill = 0)%>% 
  group_by(loc_code, loc_name)%>%
  summarise(DIGESTER = sum(DIGESTER), DEWATER = sum(DEWATER), THICKEN = sum(THICKEN), CENTRIFUGE = sum(CENTRIFUGE), `SLUDGE STORAGE` = sum(`SLUDGE STORAGE`), `SLUDGE TANK` = sum(`SLUDGE TANK`), LAGOON = sum(LAGOON))

# join by loc code to address_code first four digits (might be wrong - cluster instead of facility).  
#dt_maximoGeoLocation had 281 rows; dt_maximoAssets_wwSolidsLoc had 103
dt_maximoAssets_wwSolidsLoc <-dt_maximoAssets_wwSolidsLoc %>%
inner_join(dt_maximoGeoLocation, by = "loc_code")

#N.B. problems - now forced squish by loc_code only
# Row 29 of `x` matches multiple rows in `y`.
# 5524-WWCA (Callander WWWTL) to 5524 (Callander Lagoon) 
# 5613 Southampton is misattributed - 5313 WWSH vs 5313 SPP1 (WWSH doesn't exist)

# Convert each row to JSON-like format
#json_AssetsGeoLoc <- apply(dt_maximoAssets_wwSolidsLoc, 1, function(row) {
#  paste0("[", paste(sprintf('"%s"', row), collapse = ", "), "]")
#})

# Convert each row to JSON-like format, replacing NA with null
json_AssetsGeoLoc <- apply(filter(dt_maximoAssets_wwSolidsLoc, !is.na(lat)), 1, function(row) {
  # Replace NA with null
  row <- ifelse(is.na(row), "null", sprintf('"%s"', row))
  paste0("[", paste(row, collapse = ", "), "]")
})

# Combine rows into a JSON array
json_AssetsGeoLoc <- paste0("[", paste(json_AssetsGeoLoc, collapse = ", "), "]")

# Write to file for checking
writeLines(json_AssetsGeoLoc, "array_output.json")
#17 and 18 are the lat and lon (javascript counting form 0)

#this json is then manually copy & pasted into the embed snippet inside https://ocwa0.sharepoint.com/sites/OBRR/SitePages/maximoSolidsTrainAssets.aspx?Mode=Edit
```

```{r,eval=FALSE}
#convert json to  geojson 
#https://gis.stackexchange.com/questions/73756/is-it-possible-to-convert-regular-json-to-geojson

#adding layers using geojson data to leaflet
#https://courses.ems.psu.edu/geog585/node/769

## Install packages if not already installed
# install.packages("geojsonio")

library(geojsonio)
# The geojson_write function handles the conversion and file output
geojson_write(
  df,
  lat = "latitude",
  lon = "longitude",
  file = "your_output_file.geojson"
)

#then in the leaflet js 
#take the content of the converted file:
#var geoJsonPoints = [{
#"type": "FeatureCollection",
#"features": [
#{ "type": "Feature", "properties": { "name": "LocationA", "latitude": 34.0522, "longitude": -118.2437 }, "geometry": { "type": "Point", "coordinates": [ -118.2437, 34.0522 ] } },
#{ "type": "Feature", "properties": { "name": "LocationB", "latitude": 36.1628, "longitude": -115.1372 }, "geometry": { "type": "Point", "coordinates": [ -115.1372, 36.1628 ] } }
#]
#}];
#L.geoJSON(geojsonFeature).addTo(map);

#tested successfully here - https://ocwa0.sharepoint.com/sites/OBRR/SitePages/test-map.aspx?Mode=Edit
```

# Load in coordinates from PCS
```{r}
# the data file is queried with only "%WW%" to limit to ww assets 
fnames <- fs::dir_ls('data/2-Facilities/', regexp = 'Master-List')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_pcsFacilities <- suppressMessages(suppressWarnings(
  bind_rows(lapply(fnames, read_excel, col_names = TRUE, trim_ws=TRUE, skip = 1))
))

dt_pcsFacilities<-janitor::clean_names(dt_pcsFacilities)
dt_pcsFacilities$utm_e<-as.numeric(dt_pcsFacilities$utm_e)
dt_pcsFacilities$utm_n<-as.numeric(dt_pcsFacilities$utm_n)
dt_pcsFacilities$wwt<-as.factor(dt_pcsFacilities$wwt)
dt_pcsFacilities  <- dt_pcsFacilities %>% drop_na(utm_e) %>% #sf does not permit empty
  filter(!is.na(wwt)) 

#about 30 facilities missing coords
#500 facilities dropped once filtered for non-NA wwt (captures some non-registered, other facilities too)
dt_pcsFacilities$wwt <- str_replace_all(dt_pcsFacilities$wwt, "Not Req.|Not Req|Not Reg|TBD", "Not req./TBD")

#Conversion of data frame to sf object
library(sf)
dt_pcsFacilities.sf<-st_as_sf(x = dt_pcsFacilities,                         
                  coords = c("utm_e", "utm_n"), remove = FALSE,
                  crs = "4326")
```

```{r, eval = FALSE}

#check works- plot(dt_pcsFacilities.sf$geometry)

ggplot() +
  geom_sf(data=dt_pcsFacilities.sf, size = 4, aes(color = wwt))+
  labs(title = "Facilities") +
  scale_color_viridis_d()
```
Basic
```{r}
library(leaflet)
library(leaflet.extras2) #provides search function

pal <- leaflet::colorFactor(viridis_pal(option = "C")(5), domain = dt_pcsFacilities$wwt)

# leaflet(dt_pcsFacilities) %>% addProviderTiles("CartoDB.Positron") %>% 
#   addCircleMarkers(~utm_e, ~utm_n, color = ~pal(wwt), fill = FALSE) %>% 
#   addLegend(position = "bottomright",
#             pal=pal, values = ~wwt)
```


```{r}
#join maximo assets to pcs facilities location & info, prep for mapping

dt_pcsFacilities<-left_join(dt_pcsFacilities, dt_maximoAssets_wwSolids, by = join_by(org == loc_code))

dt_pcsFacilities<- dt_pcsFacilities %>% 
  dplyr::mutate(popupText = paste0(location_name,
                               "<br/>LocCode:", org,
                               "<br/>Type:", mecp_category))

wwIcons <- iconList(
  DIGESTER = makeIcon("https://icons.cdn.biorender.com/w75xh75/5ea0b6e3a1aad100287f0063/5ea0b5e3a1aad100287f005e.png","https://icons.cdn.biorender.com/w75xh75/5ea0b6e3a1aad100287f0063/5ea0b5e3a1aad100287f005e.png", 13, 20, iconAnchorX = 1, iconAnchorY = 1),
  "SLUDGE STORAGE" = makeIcon("https://icons.cdn.biorender.com/w75xh75/6504b75c32b65c0c6b1bdfec/63c1b74228c02200215bdab2.png","https://icons.cdn.biorender.com/w75xh75/6504b75c32b65c0c6b1bdfec/63c1b74228c02200215bdab2.png", 13,13, iconAnchorX = 10, iconAnchorY = 10),
  LAGOON = makeIcon("https://icons.cdn.biorender.com/w75xh75/5b5a4ef60bb8280014d9fa85/5b5a4d690bb8280014d9fa7c.png", "https://icons.cdn.biorender.com/w75xh75/5b5a4ef60bb8280014d9fa85/5b5a4d690bb8280014d9fa7c.png", 10,10,   iconAnchorX = -1, iconAnchorY = -1),
  CENTRIFUGE = makeIcon("https://icons.cdn.biorender.com/w75xh75/65c10cbe7ea8a97b919caebe/65c10c5c7ea8a942ff9caea1.png", "https://icons.cdn.biorender.com/w75xh75/65c10cbe7ea8a97b919caebe/65c10c5c7ea8a942ff9caea1.png", 10,10,iconAnchorX = -2, iconAnchorY = -2),
  "SLUDGE TANK" = makeIcon("https://icons.cdn.biorender.com/w75xh75/64887eff384a540020d0261f/64887e1e384a540020d0260a.png", "https://icons.cdn.biorender.com/w75xh75/64887eff384a540020d0261f/64887e1e384a540020d0260a.png", 10,10,iconAnchorX = 5, iconAnchorY = 5)
    # ifelse(quakes1$mag < 4.6, #to use for aerobic v anaerobic in future
    # "https://leafletjs.com/examples/custom-icons/leaf-green.png",
    # "https://leafletjs.com/examples/custom-icons/leaf-red.png"
  #),
)

map<-leaflet(dt_pcsFacilities) %>% addProviderTiles("CartoDB.Positron") %>% 
    leaflet::addMarkers(
    data = subset(x = dt_pcsFacilities, matched_keyword == "DIGESTER"),
    lng = ~utm_e, lat = ~utm_n,
    group = "Digester",
    icon = ~wwIcons[matched_keyword],
    options = markerOptions(opacity =0.6)
  ) %>% 
    leaflet::addMarkers(
    data = subset(x = dt_pcsFacilities, matched_keyword == "SLUDGE STORAGE"),
    lng = ~utm_e, lat = ~utm_n,
    group = "Sludge Storage",
    icon = ~wwIcons[matched_keyword],
    options = markerOptions(opacity =0.6)
  ) %>% 
    leaflet::addMarkers(
    data = subset(x = dt_pcsFacilities, matched_keyword == "SLUDGE TANK"),
    lng = ~utm_e, lat = ~utm_n,
    group = "Sludge Tank",
    icon = ~wwIcons[matched_keyword],
    options = markerOptions(opacity =0.6)
  ) %>% 
    leaflet::addMarkers(
      data = subset(x = dt_pcsFacilities, matched_keyword == "LAGOON"),
      lng = ~utm_e, lat = ~utm_n,
      group = "Lagoon",
      icon = ~wwIcons[matched_keyword],
      options = markerOptions(opacity =0.6)
    ) %>% 
    leaflet::addMarkers(
      data = subset(x = dt_pcsFacilities, matched_keyword == "CENTRIFUGE"),
      lng = ~utm_e, lat = ~utm_n,
      group = "Centrifuge",
      icon = ~wwIcons[matched_keyword],
      options = markerOptions(opacity =0.6)
    ) %>% 
  # controlling the groups
  leaflet::addLayersControl(
    overlayGroups = c("Digester", "Sludge Storage", "Sludge Tank", "Centrifuge", "Lagoon"),  # add these layers
    options = layersControlOptions(collapsed = FALSE)
    ) %>%   # expand on hover? 
  addCircleMarkers(~utm_e, ~utm_n, group = 'facilities',
                   color = ~pal(wwt), weight = 1, radius = 6, fill = FALSE,
                   popup = ~as.character(popupText)) %>% 
  addLegend(position = "bottomright", title = "wastewater facility class",
            pal=pal, values = ~wwt)

map <-map %>% addGeosearch()

map
```




```{r}
#export
library(htmlwidgets)
saveWidget(map, file = "PCSwwtf_MaximoSolidsAssets.html")#, selfcontained = TRUE)
```

# WISKI recorded stabilization method and end destinations

```{r}
#read in metadata file
dt_wiskiMeta<-read_excel('data/2-Facilities/All_WTF_WWTF_Metadata.xlsx', col_names = TRUE, trim_ws=TRUE) 

#reducing down to fields of interest (35)
dt_wiskiMeta<-dt_wiskiMeta %>% 
  select(site_name, orgUnit, RegionList, ends_with(c("Capacity", "Flow")), starts_with(c("SludStabz", "SludgeDisp")))

#unit conversion (#1343 is wrong 60 and 60000L/day)
dt_wiskiMeta<-dt_wiskiMeta %>% 
  mutate(`Total Design Capacity`= if_else(`Unit Total Capacity`== "L/day", `Total Design Capacity`/1000, `Total Design Capacity`))%>% 
    mutate(`Total Design Capacity`=if_else(`Unit Total Capacity`== "ML/day", `Total Design Capacity`*1000, `Total Design Capacity`)) %>% 
    mutate(`Total Design Capacity`=if_else(`Unit Total Capacity`== "1000m3/day", `Total Design Capacity`*1000, `Total Design Capacity`)) 

dt_wiskiMeta$`Unit Total Capacity`<-str_replace_all(dt_wiskiMeta$`Unit Total Capacity`, "L/day|ML/day|1000m3/day", "m3/day")

dt_wiskiMeta<-dt_wiskiMeta %>% 
  mutate(`Design Average Day Flow`= if_else(`Unit Average Day Flow`== "L/day", `Design Average Day Flow`/1000, `Design Average Day Flow`))%>% 
    mutate(`Design Average Day Flow`=if_else(`Unit Average Day Flow`== "ML/day", `Design Average Day Flow`*1000, `Design Average Day Flow`)) 

dt_wiskiMeta$`Unit Average Day Flow`<-str_replace_all(dt_wiskiMeta$`Unit Average Day Flow`, "L/day|ML/day", "m3/day")

#if field has one, then fill it with split second part
#anything starting with SludStabz or SludgeDisp, split by _
#https://stackoverflow.com/questions/50138295/replace-column-values-with-column-name-using-dplyrs-transmute-all
df <- dt_wiskiMeta %>%
  mutate(across(
    starts_with("Slud"),  
    ~ str_replace(., "1",~ cur_column() )  # Replace first match boolean-exists with column name
  ))

df<-df  %>% 
  pivot_longer(
    cols = starts_with("SludStabz"), 
#    names_to = "Stabilization", 
#    names_prefix = "SludStabz",
#    names_transform = as.integer,
    values_to = "Stabilization"
#    values_drop_na = TRUE,
  )

#pivot stabilization columns longer first - values will be both NA and 1's
#only one parsing failure reported - row 17 St. Charles? to look into
dt_wiskiMeta_stabilization<-dt_wiskiMeta  %>%
  pivot_longer(
    cols = starts_with("SludStabz"),
    names_to = c("Stabilization"), 
    names_pattern = "SludStabz_?(.*)",
    names_transform = list(
      Stabilization = ~ readr::parse_factor(.x, levels = c("AerobicDigest-Mul","AerobicDigest-1","Aerobic Digest-2","AnaerobicDigest-Mul","AnaerobicDigest-1","AnaerobicDigest-2","Decanting" ,"DecantingCentrifuge","Dewatering_Gravity","Dewatering_VacFilt","No Stabilization","Partial_Stabilized","SludgeDewatering",  "SludgeDryingBeds","SludgeThickening","Tilled"))
    ),
    values_to = "StabilizationBoolean"
)

#NB second pivot
#pivot destination columns longer - values will be both NA and 1's
#55k rows result
dt_wiskiMeta_EndDestination<-dt_wiskiMeta %>%
  pivot_longer(
    cols = starts_with("SludgeDisp"),
    names_to = c("EndDestination"), 
    names_pattern = "SludgeDisp_?(.*)",
    names_transform = list(
      EndDestination = ~ readr::parse_factor(.x, levels = c("Agricultural Land","Blended By Hauler","Dewatering","Drying Bed(S)","Incineration","Lagoon Storage","Landfill","Rem_Hauler2CertifRec", "Stabilize_HaulerLagoon","Transfer2AnotherFac","Not Applicable"))
    ),
    values_to = "EndDestBoolean"
)

#Replace Boolean columns with corresponding string description
#Drop row which are NA in BOTH Boolean columns, or are just interim like dewatering and drying
dt_wiskiMeta_EndDestination <- dt_wiskiMeta_EndDestination %>%
  mutate(EndDestBoolean = if_else(EndDestBoolean==1, EndDestination, NA)) %>% 
  filter(!(is.na(EndDestBoolean))) %>%
  filter(!(EndDestination == "Dewatering")) %>% 
  filter(!(EndDestination == "Drying Bed(S)"))

# Calculate counts and percentages - works but not really useful as it's % of counts not volumes as one'd naturally expect
# dt_wiskiMeta_EndDestinationSummary<-dt_wiskiMeta_EndDestination %>%
#   count(EndDestination) %>%
#   mutate(percentageEndDestination = n / sum(n) * 100)


# needs working on to combine end dest and stabilization
df <- df %>%
  mutate(EndDestBoolean = if_else(EndDestBoolean==1, EndDestination, NA)) %>% 
  mutate(StabilizationBoolean = if_else(StabilizationBoolean==1, Stabilization, NA)) %>% 
  filter(!(is.na(StabilizationBoolean)& is.na(EndDestBoolean))) 

```

```{r}
# bring over wiski snapshot to show
# waffle valume map
# what level of dryness where and what type of technology (use maximo not wiski...)
```

