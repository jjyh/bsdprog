---
title: "WISKIsnapshot"
author: "Jane Ho"
format: html
---

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(leaflet)
library(janitor)
library(tidyr)
library(dplyr)
library(readxl)
library(purrr)
library(ggplot2)
library(viridis)
library(stringr)
library(lubridate)
library(svglite)

options(scipen = 999) 
```

```{r}
#create differentiated version for export
currentIssue<-"2025T2"
currentYear<-"2025"
currentTrimester<- "August" #end month
```


# Structure
- One contiguous file with 3 years data - read & keep stitching onto a datatable, discard any older entries (can always manually re-spec to include timeframe later)
- parse down to (start now w/) one quarter - OR does it make more sense to to land app season vs. winter (roughly semi-annual, but more delay)
- start with highlights (manual & semi-automated text); future -- comparatives with previous year quarter

## seperate endeavour
rough mass accounting via steps in wwtp - however will likely need to be unique for each plant (maybe set up for Stratford as test)

# import
```{r}
#grab all excel names as a character vector, assuming multiple quarterly files will be available.  may need more precision if non wiski files are stored

fnames <- fs::dir_ls('data/2-Facilities/', regexp = 'All_WTF_WWTF_Metadata.xlsx')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_wiskiMeta <- bind_rows(lapply(fnames, read_excel, sheet = "Wastewater", col_names = TRUE, trim_ws=TRUE))

#report number of rows
print(nrow(dt_wiskiMeta))


```


```{r}
#grab all excel names as a character vector, assuming multiple quarterly files will be available.  may need more precision if non wiski files are stored

fnames <- fs::dir_ls('data/3-Inventory/', regexp = '.xlsx')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_wiskiBSD <- bind_rows(lapply(fnames, read_excel, sheet = "IH_OL_data", col_names = TRUE, trim_ws=TRUE, col_types = c("text", "guess", "guess",  "guess", "guess", "guess", "guess", "guess", "guess", "guess", "guess"))) #have to spec first site_no as text otherwise it'll guess for numbers and create NAs.  fragile for other columns since Aziz adds manually

#report number of rows
print(nrow(dt_wiskiBSD))
```


# Haulage
Amounts are variously reported under the four parameters: **HaulVol, HaulMass, HaulMassOffSite, and HauledVol** (of which HaulMass only exists in one facility, 5271), with the units either as kg, cubic meter per day, or cubic meter.  The export produces daily values, therefore all values are converted to cubic meter associated with the datestamp.

```{r}
#reduce & normalize dataset
dt_wiskiHaul<-dt_wiskiBSD %>%
  filter(grepl("Haul", parameter)) %>% 
  filter(DailyValue >0 & !is.na(DailyValue)) %>% 
  mutate(Value_m3 = case_when(
    unit == "m続" ~ DailyValue,
    unit == "m続/d"  ~ DailyValue,
    unit == "kg" ~ DailyValue/1000)) %>% 
#StationName will hint which process it came out of (but remove North, South, numericals, and Quality)
  mutate(StationName = str_replace_all(StationName, "[:digit:]", "")) %>% 
  mutate(StationName = str_replace_all(StationName, c("North" = "", "South" = "", "Quality"= "", "data" = ""))) %>% 
    mutate(StationName = str_replace_all(StationName, c("  " = " "))) %>% 
  mutate(StationName = str_trim(StationName))

#prep date aggregation
dt_wiskiHaul$month <- as.Date(cut(dt_wiskiHaul$Date,breaks = "month"))
dt_wiskiHaul$week <-  isoweek(ymd(dt_wiskiHaul$Date))
```

keep sites - later associate w hub

Daily Haulage
provide cluster option
```{r}
dt_wiskiHaul %>% 
  ggplot(aes(x= Date, y = Value_m3)) + #aes b/c will update will be based on variable
  geom_bar(stat = 'identity', aes(fill = StationName))+
  #geom_text(aes(label=Value_m3), vjust= -0.3, size = 3.5)+
  theme_minimal()+
  scale_fill_viridis(discrete = TRUE)

```

```{r}
#aggregate by week - only for geom_text d/s but may be not needed

dt_wiskiHaulWeekly<-
  aggregate(Value_m3~week(Date), data = dt_wiskiHaul, sum)

```

```{r}
p_haulage<-ggplot(data = dt_wiskiHaul, aes(x=week, y=Value_m3 )) + #aes b/c will update will be based on variable
  geom_bar(stat = 'identity', aes(fill = StationName))+
  #geom_text(data=dt_wiskiHaulWeekly, aes(label=Value_m3), vjust= -0.3, size = 3.5)+
  theme_minimal()+
  scale_fill_viridis(discrete = TRUE)+
  scale_x_continuous(breaks=seq(0,53,1) ) +
  labs(x = "Week Number",
       y = "Weekly Volume (m3)",
       title = paste("Haulage recorded in WISKI",  currentYear, "up to", currentTrimester),
       caption = "(data recorded in kg are divided by 1000 [rough conversion - majority report in cubic meters]) \n minor StationName changes: removed subpartitions & superfluous: numbers, north/south, 'data', 'quality' ")
  #+scale_x_date(date_breaks = "1 month", date_labels = "%b")

ggsave(file=paste("export/", currentIssue, "/p_haulage.svg", sep=""), plot=p_haulage, width=16, height=9)

#https://forum.posit.co/t/plot-weekly-data-with-monthly-axis-labels/62631/5
```

Quality - try TS first
range - boxplot amongst plants
pane or filter by process (rely on 2019 canvassing [?] unless metadata shows process changed)

```{r}

fnames <- fs::dir_ls('data/3-Inventory/', regexp = '.xlsx')
fnames <- fnames[!grepl("~", fnames)] #remove any temp files with ~
dt_wiskiLabBSD <- bind_rows(lapply(fnames, read_excel, sheet = "Lab_data", col_names = TRUE, trim_ws=TRUE))

#report number of rows
print(nrow(dt_wiskiLabBSD))

#!! N.B. caution from Aziz:
# sta_num_shortname added manually (not in export)
# station number is a combo of works number and the station # shortname (latter used by lab to upload)
# WA_result_value_unit is the correct value (do NOT use param_unit which is what gets converted by WISKI)  

#reduce & normalize dataset
#exclude Bpd (primary digestion) and Brs (raw sludge), assuming all are in Bslq*, Bsd*, Bth*

dt_wiskiLabBSD<-dt_wiskiLabBSD %>%
  filter(grepl("Bsd|Bslq|Bth", Station_no, ignore.case=T)) %>% 
  filter(grepl("TS", Parameter_Number))%>% #ask Aziz - why is one parameter and the other Parameter_Number
  #note this will grab TSS - but plants reporting TSS at these stations are reporting v. high, borderline max (20000mg/L) method values, suspect they may be TS - include & call out.  There's also 1 or 2 plants reporting MLSS on these stations...leave for now
  #units are almost all mg/L, a few %
  #assume specific gravity of water and biosolids are both equal to one (holds true only when the solids concentration in the biosolids is low), 1% TS = 10,000mg/L
  mutate("Value_mgPerL" = case_when(
    WQ_result_value_unit == "mg/L" ~ Result_Value,
    WQ_result_value_unit == "%"  ~ Result_Value *10000,
    WQ_result_value_unit == "mg/kg"  ~ Result_Value)
    ) 

```

```{r}
#plant specific customizations
# Paris contains liquid biosolids (bslq) and dewatered (bslq1)
# Belleville and others also contains multiple stages
dt_wiskiLabBSD<-dt_wiskiLabBSD %>%
  filter(Station_no!="110001097-Bslq")%>% #Paris liquid biosolids
  filter(Station_no!="110000016-Brs")%>% #Belleville raw
  filter(Station_no!="110000016-Bpd")%>% #Belleville digester
  filter(Parameter_Number!="TSulphur") %>% #Belleville sulphur gets wrapped in TS otherwise
  filter(Station_no!="110000980-Bslq")%>% #Deseronto digester sludge (vs. 2 geotubes as bslq2 and 3)
  filter(Station_no!="110000873-Bth") #mississipi mills cake (vs. SNDR and ATAD)
  
```


```{r}
#medians by Station_no
(unique(dt_wiskiLabBSD$'Station Name'))
(dt_wiskiLabBSD$`Value mg/L`)

p_TS<-dt_wiskiLabBSD %>%
  ggplot(aes(x=site_Name, y=Value_mgPerL)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=5))+
    labs(y = "Values (mg/L)",
         x= "",
       title = paste("Total Solids in Biosolids (Blsq, Bth, Bsd) stations recorded in WISKI, up to", currentTrimester, currentYear),
       caption = "(data recorded in % and mg/kg are divided by 10000, taken on par, respectively, assuming water density  \n Rockland and Espanola record MLSS at biosolids stations and are excluded \n Plants recording TSS on biosolids are assumed to be equiv. to TS")

p_TS
ggsave(file=paste("export/", currentIssue, "/p_TS.svg", sep=""), plot=p_TS, width=16, height=9)
library(plotly)
ggplotly(p_TS)

```

# Normalization
Use effluent ("treated water") to be consistent with HBA.  N.B. some plants won't have effluent meter & uses influent in-stead.  Run data gap check.
```{r}
#reduce & normalize dataset
dt_wiskiEff<-dt_wiskiBSD %>%
  filter(grepl("Flow", parameter)) %>% 
  #discard all numerical as they seem to be sub meters/sub flows which get summed
  filter(!grepl("[0-9]", parameter)) %>%
  filter(!grepl("[0-9]", StationName)) %>%
  filter(DailyValue >0 & !is.na(DailyValue)) %>% 
  mutate(Value_m3 = case_when(
    unit == "m続" ~ DailyValue,
    unit == "m続/d"  ~ DailyValue,
    unit == "Ml/d"  ~ DailyValue*1000, #CLK
    unit == "l/d" ~ DailyValue/1000)) 

dt_wiskiEff <- dt_wiskiEff %>% group_by(site_no, site_Name) %>% 
  summarize(
    EffluentCumulative_m3 = round(sum(Value_m3), digits = 0)
  )

# to be added - a duplicate scrub irregardless of parameter and station
# check Amherstburg - crazy high
#site 5047 has Flow-1 and Flow-2 adding up to Flow
#5302 has Flow-1, Flow1-1
#5901 has Flow-Total_of_Zones and Flow which are the same and double counted
#5867 has Flow and Flow: Total of All Sources which are duplicates and indistinguishable (IH Edited)

# drop where we're not reporting lab values
dt_wiskiLabBSDEff<- left_join(dt_wiskiLabBSD, dt_wiskiEff, by = "site_Name") %>% 
    drop_na#there's a handful of sites with bsd labs but no flows

# Calculate quartile boundaries
quartiles <- quantile(dt_wiskiLabBSDEff$EffluentCumulative_m3, probs = c(0, 0.25, 0.5, 0.75, 1))

dt_effQuartileRanges <- data.frame(
  Quartile = c("0-25%", "25-50%", "50-75%", "75-100%"),
  Range = paste("facilities with", paste(paste(head(quartiles, -1), tail(quartiles, -1), sep = " - "), "m3 effluent up to ", currentTrimester, " ", currentYear, sep = "")), sep=""
)

dt_wiskiLabBSDEff$effQuartile <- cut(dt_wiskiLabBSDEff$EffluentCumulative_m3, 
                   breaks = quartiles, 
                   include.lowest = TRUE, 
                   labels = dt_effQuartileRanges$Range)

```

```{r}
# dt_wiskiLabBSDEff<- left_join(dt_wiskiLabBSD, dt_wiskiEff, by = "site_Name") %>% 
#     drop_na#there's a handful of sites with bsd labs but no flows
```


```{r}
# faceted by quartile of effluent 
p_TSsplit<- dt_wiskiLabBSDEff %>% 
  ggplot(aes(x=site_Name, y=Value_mgPerL)) +
    geom_boxplot() +
    scale_fill_viridis(discrete = TRUE, alpha=0.6) +
    geom_jitter(color="black", size=0.4, alpha=0.9) +
    theme(axis.text.x  = element_text(angle=90, vjust=0.5, size=5))+
    labs(y = "Values (mg/L)",
         x= "",
       title = paste("Total Solids in Biosolids (Blsq, Bth, Bsd) stations recorded in WISKI,", currentYear, "up to", currentTrimester),
       caption = "(data recorded in % and mg/kg are divided by 10000, taken on par, respectively, assuming water density  \n Rockland and Espanola record MLSS at biosolids stations and are excluded \n Plants recording TSS on biosolids are assumed to be equiv. to TS") +
  theme(axis.text.x = element_text(angle = 45))+
  facet_wrap(~effQuartile, scales = "free_y", ncol = 1)+
  coord_flip()

p_TSsplit

ggsave(file=paste("export/", currentIssue, "/p_TSsplit.svg", sep=""), plot=p_TSsplit, width=16, height=9)
```


Stats summary
```{r}
#what is my production rate? by hub

dt_wiskiHaul<- left_join(dt_wiskiHaul, dt_wiskiMeta[,c(1:17)], by = c("site_no" = "orgUnit")) 

dt_wiskiHaul <- dt_wiskiHaul %>% group_by(RegionList) %>% 
  summarize(
    HauledCumulative_m3 = round(sum(Value_m3), digits = 0)
  )

dt_wiskiEff<- left_join(dt_wiskiEff, dt_wiskiMeta[,c(1:17)], by = c("site_no" = "orgUnit"))
dt_wiskiEff <- dt_wiskiEff %>% group_by(RegionList) %>%
  summarize(
    EffluentCumulative_m3 = round(sum(EffluentCumulative_m3), digits = 0)
  )

dt_wiskiProd<-left_join(dt_wiskiEff, dt_wiskiHaul, by = "RegionList")

```

# Manual things
Do these on embedded excel

## Mass linkage
- e.g. Waterloo receiving from Kitchener, Preston from Galt : these may be double-counted until the lineage is sorted
- leachate - mass email from Jisna out to RHMs

facet by region - will help RHM, when looking at GP $

facet by flow range? or population served will auto-correlate BUT population isn't going to be updated

facet by class? - no - inaccuracies

cost per hub - Witte's sheet divided by contract durations
